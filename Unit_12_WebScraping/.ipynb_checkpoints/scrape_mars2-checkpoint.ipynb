{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "# def scrape_all():\n",
    "\n",
    "# Initiate headless driver for deployment\n",
    "browser = Browser(\"chrome\", executable_path=\"chromedriver\", headless=False)\n",
    "# news_title, news_paragraph = mars_news(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all scraping functions and store in dictionary.\n",
    "data = {\n",
    "    \"news_title\": news_title,\n",
    "    \"news_paragraph\": news_paragraph,\n",
    "    \"featured_image\": featured_image(browser),\n",
    "    \"hemispheres\": hemispheres(browser),\n",
    "    \"weather\": twitter_weather(browser),\n",
    "    \"facts\": mars_facts(),\n",
    "    \"last_modified\": dt.datetime.now()\n",
    "}\n",
    "\n",
    "# Stop webdriver and return data\n",
    "browser.quit()\n",
    "return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-4-ea5591ffdf63>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ea5591ffdf63>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    return None, None\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# # NASA MARS NEWS\n",
    "\n",
    "# def mars_news(browser):\n",
    "\n",
    "# URL of page to be scraped\n",
    "url='https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(2)\n",
    "# Get first list item and wait half a second if not immediately present\n",
    "browser.is_element_present_by_css(\"ul.item_list li.slide\")\n",
    "\n",
    "# Scrape page into Soup\n",
    "html=browser.html\n",
    "news_soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "try:\n",
    "    slide_elem = news_soup.select_one(\"ul.item_list li.slide\")\n",
    "\n",
    "    # Find headline and summary\n",
    "    news_title=slide_elem.find('div',class_='content_title').find('a').text\n",
    "    news_p=slide_elem.find('div',class_='article_teaser_body').text\n",
    "\n",
    "except AttributeError:\n",
    "    return None, None\n",
    "\n",
    "print(news_title, news_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# # JPL MARS SPACE IMAGES\n",
    "def featured_image(browser):\n",
    "    # Visit url\n",
    "    jpl_mars_images_url='https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(jpl_mars_images_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Scrape page into soup\n",
    "    html=browser.html\n",
    "    img_soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    try:\n",
    "        #Find URL for featured image\n",
    "        featured_image_url=img_soup.find('article')['style'].replace('background-image: url(','').replace(');','')\n",
    "\n",
    "        #\"Unpack URL\" - remove ''\n",
    "        featured_image_url=featured_image_url[1:-1]\n",
    "\n",
    "        #Base URL\n",
    "        jpl_url='https://www.jpl.nasa.gov'\n",
    "\n",
    "        #Base URL + Image URL\n",
    "        featured_image_url=jpl_url+featured_image_url\n",
    "\n",
    "    except AttributeError:\n",
    "        return None\n",
    "    \n",
    "    return featured_image_url\n",
    "\n",
    "# # MARS WEATHER\n",
    "def twitter_weather(browser):\n",
    "    #Visit url\n",
    "    mars_weather_twitter='https://twitter.com/marswxreport?lang=en'\n",
    "    browser.visit(mars_weather_twitter)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Scrape into Soup\n",
    "    html=browser.html\n",
    "    weather_soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    #Gather weather data from latest tweet\n",
    "    tweets=weather_soup.find('p',class_='TweetTextSize').text\n",
    "    #Remove excess information\n",
    "    mars_weather=tweets.replace('InSight','')\n",
    "\n",
    "    return mars_weather\n",
    "\n",
    "# # MARS FACTS\n",
    "def mars_facts():\n",
    "\n",
    "    #Visit url\n",
    "    mars_facts_url='https://space-facts.com/mars/'\n",
    "    \n",
    "    time.sleep(2)\n",
    "    try:\n",
    "        #Read tables\n",
    "        tables=pd.read_html(mars_facts_url)\n",
    "\n",
    "        #Assign variable to first table\n",
    "        mars_profile_df=tables[0]\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    #Format table\n",
    "    mars_profile_df.columns=['Description','Value']\n",
    "    mars_profile_df.set_index('Description',inplace=True)\n",
    "\n",
    "    #Transform table into html\n",
    "    return mars_profile_df.to_html(classes='table-striped')    \n",
    "\n",
    "# # MARS HEMISPHERES\n",
    "def hemispheres(browser):\n",
    "    #Visit url\n",
    "    mars_hemisphere_url='https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    \n",
    "    browser.visit(mars_hemisphere_url)\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    hemisphere_image_urls = []\n",
    "    \n",
    "    for i in range(4):\n",
    "\n",
    "        # Find the elements on each loop to avoid a stale element exception\n",
    "        browser.find_by_css(\"a.product-item h3\")[i].click()\n",
    "\n",
    "        hemi_data = scrape_hemisphere(browser.html)\n",
    "\n",
    "        # Append hemisphere object to list\n",
    "        hemisphere_image_urls.append(hemi_data)\n",
    "\n",
    "        # Finally, we navigate backwards\n",
    "        browser.back()\n",
    "\n",
    "    return hemisphere_image_urls\n",
    "\n",
    "def scrape_hemisphere(html_text):\n",
    "\n",
    "    # Soupify the html text\n",
    "    hemi_soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    # Try to get href and text except if error.\n",
    "    try:\n",
    "        title_elem = hemi_soup.find(\"h2\", class_=\"title\").get_text()\n",
    "        sample_elem = hemi_soup.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        # Image error returns None for better front-end handling\n",
    "        title_elem = None\n",
    "        sample_elem = None\n",
    "\n",
    "    hemisphere = {\n",
    "        \"title\": title_elem,\n",
    "        \"img_url\": sample_elem\n",
    "    }\n",
    "\n",
    "    return hemisphere\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
